{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8372e54b-6196-461a-a470-822b0c458fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "N, D = 3, 4\n",
    "\n",
    "x = np.random.randn(N, D)\n",
    "y = np.random.randn(N, D)\n",
    "z = np.random.randn(N, D)\n",
    "\n",
    "a = x * y\n",
    "b = a + z\n",
    "c = np.sum(b)\n",
    "\n",
    "grad_c = 1.0\n",
    "grad_b = grad_c * np.ones((N, D))\n",
    "grad_a = grad_b.copy()\n",
    "grad_z = grad_b.copy()\n",
    "grad_x = grad_a * y\n",
    "grad_y = grad_a * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "960a6d41-d550-4f08-bc6d-bef1e25c7c3c",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: invalid device ordinal\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-da3374752df1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mD\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mD\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mD\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mD\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: invalid device ordinal\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = 'cuda:1'\n",
    "N, D = 3, 4\n",
    "x = torch.randn(N, D, requires_grad=True)\n",
    "y = torch.randn(N, D, device=device)\n",
    "z = torch.randn(N, D, device=device)\n",
    "\n",
    "a = x * y\n",
    "b = a + z\n",
    "c = torch.sum(b)\n",
    "\n",
    "c.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03037a44-2cdf-4166-bdb7-f32ee9d14003",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device('cpu')\n",
    "\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "x = torch.randn(N, D_in, device=device)\n",
    "y = torch.randn(N, D_out, device=device)\n",
    "w1 = torch.randn(D_in, H, device=device)\n",
    "w2 = torch.randn(H, D_out, device=device)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    h = x.mm(w1)\n",
    "    h_relu = h.clamp(min=0)\n",
    "    y_pred = h_relu.mm(w2)\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    \n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.t().mm(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.mm(w2.t())\n",
    "    grad_h = grad_h_relu.clone()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = x.t().mm(grad_h)\n",
    "    \n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f244fa43-092d-42f3-8fd8-732ee2b5effe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "w1 = torch.randn(D_in, H, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * grad_w1\n",
    "        w2 -= learning_rate * grad_w2\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8bc5b99a-d7cc-44fd-b587-79e6f7239f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyReLU(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x):\n",
    "        ctx.save_for_backward(x)\n",
    "        return x.clamp(min=0)\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_y):\n",
    "        x, = ctx.saved_tensors\n",
    "        grad_input = grad_y.clone()\n",
    "        grad_input[x < 0] = 0\n",
    "        return grad_input\n",
    "    \n",
    "def my_relu(x):\n",
    "    return MyReLU.apply(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9ae5bbf-b44d-4349-b76c-a41dd4194846",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "w1 = torch.randn(D_in, H, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    y_pred = my_relu(x.mm(w1)).mm(w2)\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * grad_w1\n",
    "        w2 -= learning_rate * grad_w2\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d9beae7-d191-4c1b-bae7-6394553bf7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "            torch.nn.Linear(D_in, H),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(H, D_out))\n",
    "\n",
    "learning_rate = 1e-2\n",
    "for t in range(500):\n",
    "    y_pred = model(x)\n",
    "    loss = torch.nn.functional.mse_loss(y_pred, y)\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            param -= learning_rate * param.grad\n",
    "    model.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f28a4fa5-e0ff-4b28-92a8-cefaa438f61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "            torch.nn.Linear(D_in, H),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(H, D_out))\n",
    "\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "for t in range(500):\n",
    "    y_pred = model(x)\n",
    "    loss = torch.nn.functional.mse_loss(y_pred, y)\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb87bdf8-0909-4c2d-ba3a-253afd1ecc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class TwoLayerNet(torch.nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        super(TwoLayerNet, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(D_in, H)\n",
    "        self.linear2 = torch.nn.Linear(H, D_out)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h_relu = self.linear1(x).clamp(min=0)\n",
    "        y_pred = self.linear2(h_relu)\n",
    "        return y_pred\n",
    "\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "model = TwoLayerNet(D_in, H, D_out)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-4)\n",
    "for t in range(500):\n",
    "    y_pred = model(x)\n",
    "    loss = torch.nn.functional.mse_loss(y_pred, y)\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2979c031-2df2-4a74-9fbb-5d75b5191118",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class ParallelBlock(torch.nn.Module):\n",
    "    def __init__(self, D_in, D_out):\n",
    "        super(ParallelBlock, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(D_in, D_out)\n",
    "        self.linear2 = torch.nn.Linear(D_in, D_out)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h1 = self.linear1(x)\n",
    "        h2 = self.linear2(x)\n",
    "        return (h1 * h2).clamp(min=0)\n",
    "\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "            ParallelBlock(D_in, H),\n",
    "            ParallelBlock(H, H),\n",
    "            torch.nn.Linear(H, D_out))\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "for t in range(500):\n",
    "    y_pred = model(x)\n",
    "    loss = torch.nn.functional.mse_loss(y_pred, y)\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b7c26127-a325-4445-9006-a27063753e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "loader = DataLoader(TensorDataset(x, y), batch_size=8)\n",
    "model = TwoLayerNet(D_in, H, D_out)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-2)\n",
    "for epoch in range(20):\n",
    "    for x_batch, y_batch in loader:\n",
    "        y_pred = model(x_batch)\n",
    "        loss = torch.nn.functional.mse_loss(y_pred, y_batch)\n",
    "    \n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105a2d0d-4fe9-41eb-b223-671e28d65ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "alexnet = torchvision.models.alexnet(pretrained=True)\n",
    "vgg16 = torchvision.models.vgg16(pretrained=True)\n",
    "resnet101 = torchvision.models.resnet101(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2aee993e-a0db-4ae3-b094-1ccccfdb28f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "w1 = torch.randn(D_in, H, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    \n",
    "    loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1eabd0a-4d5e-4cb7-b3e6-a34e3b4db476",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = build_graph()\n",
    "\n",
    "for x_batch, y_batch in loader:\n",
    "    run_graph(graph, x=x_batch, y=y_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "08096777-1bd2-4a15-abad-ae6bf4672da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "N, D, H = 64, 1000, 100\n",
    "\n",
    "x = tf.convert_to_tensor(np.random.randn(N, D), np.float32)\n",
    "y = tf.convert_to_tensor(np.random.randn(N, D), np.float32)\n",
    "w1 = tf.Variable(tf.random.uniform((D, H))) # weights\n",
    "w2 = tf.Variable(tf.random.uniform((H, D))) # weights\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    h = tf.maximum(tf.matmul(x, w1), 0)\n",
    "    y_pred = tf.matmul(h, w2)\n",
    "    diff = y_pred - y\n",
    "    loss = tf.reduce_mean(tf.reduce_sum(diff ** 2, axis=1))\n",
    "gradients = tape.gradient(loss, [w1, w2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aca62895-1135-488c-b767-8f16fac58f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "N, D, H = 64, 1000, 100\n",
    "\n",
    "x = tf.convert_to_tensor(np.random.randn(N, D), np.float32)\n",
    "y = tf.convert_to_tensor(np.random.randn(N, D), np.float32)\n",
    "w1 = tf.Variable(tf.random.uniform((D, H))) # weights\n",
    "w2 = tf.Variable(tf.random.uniform((H, D))) # weights\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(50):\n",
    "    with tf.GradientTape() as tape:\n",
    "        h = tf.maximum(tf.matmul(x, w1), 0)\n",
    "        y_pred = tf.matmul(h, w2)\n",
    "        diff = y_pred - y\n",
    "        loss = tf.reduce_mean(tf.reduce_sum(diff ** 2, axis=1))\n",
    "    gradients = tape.gradient(loss, [w1, w2])\n",
    "    w1.assign(w1 - learning_rate * gradients[0])\n",
    "    w2.assign(w2 - learning_rate * gradients[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2159cc2a-b99c-4606-9b66-6fd63a0cbc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "N, D, H = 64, 1000, 100\n",
    "\n",
    "x = tf.convert_to_tensor(np.random.randn(N, D), np.float32)\n",
    "y = tf.convert_to_tensor(np.random.randn(N, D), np.float32)\n",
    "w1 = tf.Variable(tf.random.uniform((D, H))) # weights\n",
    "w2 = tf.Variable(tf.random.uniform((H, D))) # weights\n",
    "\n",
    "optimizer = tf.optimizers.SGD(1e-6)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(50):\n",
    "    with tf.GradientTape() as tape:\n",
    "        h = tf.maximum(tf.matmul(x, w1), 0)\n",
    "        y_pred = tf.matmul(h, w2)\n",
    "        diff = y_pred - y\n",
    "        loss = tf.reduce_mean(tf.reduce_sum(diff ** 2, axis=1))\n",
    "    gradients = tape.gradient(loss, [w1, w2])\n",
    "    optimizer.apply_gradients(zip(gradients, [w1, w2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "21147fbc-68b8-4583-9780-9178950e8cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "N, D, H = 64, 1000, 100\n",
    "\n",
    "x = tf.convert_to_tensor(np.random.randn(N, D), np.float32)\n",
    "y = tf.convert_to_tensor(np.random.randn(N, D), np.float32)\n",
    "w1 = tf.Variable(tf.random.uniform((D, H))) # weights\n",
    "w2 = tf.Variable(tf.random.uniform((H, D))) # weights\n",
    "\n",
    "optimizer = tf.optimizers.SGD(1e-6)\n",
    "\n",
    "for t in range(50):\n",
    "    with tf.GradientTape() as tape:\n",
    "        h = tf.maximum(tf.matmul(x, w1), 0)\n",
    "        y_pred = tf.matmul(h, w2)\n",
    "        diff = y_pred - y\n",
    "        loss = tf.losses.MeanSquaredError()(y_pred, y)\n",
    "    gradients = tape.gradient(loss, [w1, w2])\n",
    "    optimizer.apply_gradients(zip(gradients, [w1, w2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1d3d7ae2-a92d-488d-9560-37ea28fd1efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "N, D, H = 64, 1000, 100\n",
    "\n",
    "x = tf.convert_to_tensor(np.random.randn(N, D), np.float32)\n",
    "y = tf.convert_to_tensor(np.random.randn(N, D), np.float32)\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Dense(H, input_shape=(D, ), activation=tf.nn.relu))\n",
    "model.add(tf.keras.layers.Dense(D))\n",
    "optimizer = tf.optimizers.SGD(1e-1)\n",
    "\n",
    "losses = []\n",
    "for t in range(50):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = model(x)\n",
    "        loss = tf.losses.MeanSquaredError()(y_pred, y)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d7a6a608-6ba8-473f-b8b6-a178e38d0a70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1/1 [==============================] - 0s 150ms/step - loss: 1.1599\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.1562\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.1525\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.1490\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.1455\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.1421\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.1388\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.1356\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.1324\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.1293\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.1263\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.1233\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.1204\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.1176\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.1148\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.1121\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.1094\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.1068\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.1042\n",
      "Epoch 20/50\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.1017\n",
      "Epoch 21/50\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0993\n",
      "Epoch 22/50\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0969\n",
      "Epoch 23/50\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0945\n",
      "Epoch 24/50\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0922\n",
      "Epoch 25/50\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0899\n",
      "Epoch 26/50\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0877\n",
      "Epoch 27/50\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0855\n",
      "Epoch 28/50\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0833\n",
      "Epoch 29/50\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0812\n",
      "Epoch 30/50\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0792\n",
      "Epoch 31/50\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0771\n",
      "Epoch 32/50\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0751\n",
      "Epoch 33/50\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0732\n",
      "Epoch 34/50\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0713\n",
      "Epoch 35/50\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0694\n",
      "Epoch 36/50\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0675\n",
      "Epoch 37/50\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0657\n",
      "Epoch 38/50\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0639\n",
      "Epoch 39/50\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0622\n",
      "Epoch 40/50\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0604\n",
      "Epoch 41/50\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0587\n",
      "Epoch 42/50\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0571\n",
      "Epoch 43/50\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 1.0554\n",
      "Epoch 44/50\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0538\n",
      "Epoch 45/50\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0523\n",
      "Epoch 46/50\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.0507\n",
      "Epoch 47/50\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0492\n",
      "Epoch 48/50\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0477\n",
      "Epoch 49/50\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0462\n",
      "Epoch 50/50\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.0447\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "N, D, H = 64, 1000, 100\n",
    "\n",
    "x = tf.convert_to_tensor(np.random.randn(N, D), np.float32)\n",
    "y = tf.convert_to_tensor(np.random.randn(N, D), np.float32)\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Dense(H, input_shape=(D, ), activation=tf.nn.relu))\n",
    "model.add(tf.keras.layers.Dense(D))\n",
    "optimizer = tf.optimizers.SGD(1e-1)\n",
    "model.compile(loss=tf.keras.losses.MeanSquaredError(), optimizer=optimizer)\n",
    "\n",
    "history = model.fit(x, y, epochs=50, batch_size=N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "87fc3299-bf81-4436-8b17-1f5a48560be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "N, D, H = 64, 1000, 100\n",
    "\n",
    "x = tf.convert_to_tensor(np.random.randn(N, D), np.float32)\n",
    "y = tf.convert_to_tensor(np.random.randn(N, D), np.float32)\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Dense(H, input_shape=(D, ), activation=tf.nn.relu))\n",
    "model.add(tf.keras.layers.Dense(D))\n",
    "optimizer = tf.optimizers.SGD(1e-1)\n",
    "\n",
    "@tf.function\n",
    "def model_func(x, y):\n",
    "    y_pred = model(x)\n",
    "    loss = tf.losses.MeanSquaredError()(y_pred, y)\n",
    "    return y_pred, loss\n",
    "\n",
    "for t in range(50):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred, loss = model_func(x, y)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "721864bc-a35e-4a5f-9084-03b03069961d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "static graph :  0.05996630000026926\n",
      "dynamic graph :  0.01320739999982834\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import timeit\n",
    "\n",
    "N, D, H = 64, 1000, 100\n",
    "\n",
    "x = tf.convert_to_tensor(np.random.randn(N, D), np.float32)\n",
    "y = tf.convert_to_tensor(np.random.randn(N, D), np.float32)\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Dense(H, input_shape=(D, ), activation=tf.nn.relu))\n",
    "model.add(tf.keras.layers.Dense(D))\n",
    "optimizer = tf.optimizers.SGD(1e-1)\n",
    "\n",
    "@tf.function\n",
    "def model_static(x, y):\n",
    "    y_pred = model(x)\n",
    "    loss = tf.losses.MeanSquaredError()(y_pred, y)\n",
    "    return y_pred, loss\n",
    "\n",
    "def model_dynamic(x, y):\n",
    "    y_pred = model(x)\n",
    "    loss = tf.losses.MeanSquaredError()(y_pred, y)\n",
    "    return y_pred, loss\n",
    "\n",
    "print(\"static graph : \", timeit.timeit(lambda: model_static(x, y), number=10))\n",
    "print(\"dynamic graph : \", timeit.timeit(lambda: model_dynamic(x, y), number=10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
