{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "!pip install -q tensorflow-hub\n",
    "!pip install -q tensorflow-datasets\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "print(\"Version: \", tf.__version__)\n",
    "print(\"Eager mode: \", tf.executing_eagerly())\n",
    "print(\"Hub version: \", hub.__version__)\n",
    "print(\"GPU is\", \"available\" if tf.config.experimental.list_physical_devices(\"GPU\") else \"NOT AVAILABLE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the training set into 60% and 40%, so we'll end up with 15,000 examples\n",
    "# for training, 10,000 examples for validation and 25,000 examples for testing.\n",
    "train_data, validation_data, test_data = tfds.load(\n",
    "    name=\"imdb_reviews\", \n",
    "    split=('train[:60%]', 'train[60%:]', 'test'),\n",
    "    as_supervised=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(X_batch, n_words=500):\n",
    "    shape = tf.shape(X_batch) * tf.constant([1, 0]) + tf.constant([0, n_words])\n",
    "    Z = tf.strings.substr(X_batch, 0, n_words)\n",
    "    Z = tf.strings.lower(Z)\n",
    "    Z = tf.strings.regex_replace(Z, b\"<br\\\\s*/?>\", b\" \")\n",
    "    Z = tf.strings.regex_replace(Z, b\"[^a-z]\", b\" \")\n",
    "    Z = tf.strings.split(Z)\n",
    "    return Z.to_tensor(shape=shape, default_value=b\"<pad>\")\n",
    "\n",
    "X_example = tf.constant([\"It's a great, great movie! I loved it.\", \"It was terrible, run away!!!\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def get_vocabulary(data_sample, max_size=1000):\n",
    "    preprocessed_reviews = preprocess(data_sample).numpy()\n",
    "    counter = Counter()\n",
    "    for words in preprocessed_reviews:\n",
    "        for word in words:\n",
    "            if word != b\"<pad>\":\n",
    "                counter[word] += 1\n",
    "    return [b\"<pad>\"] + [word for word, count in counter.most_common(max_size)]\n",
    "\n",
    "get_vocabulary(X_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextVectorization(tf.keras.layers.Layer):\n",
    "    def __init__(self, max_vocabulary_size=1000, n_oov_buckets=100, dtype=tf.string, **kwargs):\n",
    "        super().__init__(dtype=dtype, **kwargs)\n",
    "        self.max_vocabulary_size = max_vocabulary_size\n",
    "        self.n_oov_buckets = n_oov_buckets\n",
    "\n",
    "    def adapt(self, data_sample):\n",
    "        self.vocab = get_vocabulary(data_sample, self.max_vocabulary_size)\n",
    "        words = tf.constant(self.vocab)\n",
    "        word_ids = tf.range(len(self.vocab), dtype=tf.int64)\n",
    "        vocab_init = tf.lookup.KeyValueTensorInitializer(words, word_ids)\n",
    "        self.table = tf.lookup.StaticVocabularyTable(vocab_init, self.n_oov_buckets)\n",
    "        \n",
    "    def call(self, inputs, input_length):\n",
    "        preprocessed_inputs = preprocess(inputs, n_words=input_length)\n",
    "        return self.table.lookup(preprocessed_inputs)\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super(TextVectorization, self).get_config()\n",
    "        config.update({\n",
    "            'max_vocabulary_size': self.max_vocabulary_size,\n",
    "            'n_oov_buckets': self.n_oov_buckets\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_vocabulary_size = 100000\n",
    "n_oov_buckets = 10\n",
    "text_vectorization = TextVectorization(max_vocabulary_size, n_oov_buckets,\n",
    "                                       input_shape=[])\n",
    "\n",
    "train_examples_batch, train_labels_batch = next(iter(train_data.batch(15000)))\n",
    "text_vectorization.adapt(train_examples_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorization.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_length = 600\n",
    "embed_size = 128\n",
    "filter_sizes = '1,2,3'\n",
    "num_filters = 1500\n",
    "vocab_size = len(text_vectorization.vocab) + n_oov_buckets\n",
    "\n",
    "input = tf.keras.layers.Input(shape=(), dtype=tf.string)\n",
    "vectorized = text_vectorization(input, input_length)\n",
    "\n",
    "embed_initer = tf.keras.initializers.RandomUniform(minval=-1, maxval=1)\n",
    "embed = tf.keras.layers.Embedding(vocab_size, embed_size,\n",
    "                               embeddings_initializer=embed_initer,\n",
    "                               input_length=input_length,\n",
    "                               name='embedding')(vectorized)\n",
    "# single channel. If using real embedding, you can set one static\n",
    "embed = tf.keras.layers.Reshape((input_length, embed_size, 1), name='add_channel')(embed)\n",
    "#embed = tf.expand_dims(embed, -1)\n",
    "pool_outputs = []\n",
    "for filter_size in list(map(int, filter_sizes.split(','))):\n",
    "    filter_shape = (filter_size, embed_size)\n",
    "    conv = tf.keras.layers.Conv2D(num_filters, filter_shape, strides=(1, 1), padding='valid',\n",
    "                               data_format='channels_last', activation='relu',\n",
    "                               kernel_initializer='glorot_normal',\n",
    "                               bias_initializer=tf.keras.initializers.constant(0.1),\n",
    "                               name='convolution_{:d}'.format(filter_size))(embed)\n",
    "    max_pool_shape = (input_length - filter_size + 1, 1)\n",
    "    pool = tf.keras.layers.MaxPool2D(pool_size=max_pool_shape,\n",
    "                                  strides=(1, 1), padding='valid',\n",
    "                                  data_format='channels_last',\n",
    "                                  name='max_pooling_{:d}'.format(filter_size))(conv)\n",
    "    pool_outputs.append(pool)\n",
    "pool_outputs = tf.keras.layers.concatenate(pool_outputs, axis=-1, name='concatenate')\n",
    "pool_outputs = tf.keras.layers.Flatten(data_format='channels_last', name='flatten')(pool_outputs)\n",
    "pool_outputs = tf.keras.layers.Dropout(0.4, name='dropout1')(pool_outputs)\n",
    "dense = tf.keras.layers.Dense(256, name='dense1')(pool_outputs)\n",
    "dense = tf.keras.layers.Dropout(0.4, name='dropout2')(dense)\n",
    "outputs = tf.keras.layers.Dense(1, name='dense2')(dense)\n",
    "model = tf.keras.models.Model(inputs=[input],outputs=[outputs])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=opt,\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\"text_cnn_no_pretraining\", save_weights_only=True, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_data.shuffle(10000).batch(128),\n",
    "                    epochs=30,\n",
    "                    validation_data=validation_data.batch(128),\n",
    "                    callbacks=[checkpoint_cb],\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"text_cnn_no_pretraining\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.evaluate(test_data.batch(32), verbose=2)\n",
    "\n",
    "for name, value in zip(model.metrics_names, results):\n",
    "  print(\"%s: %.3f\" % (name, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.saved_model.save(model, \"text_cnn_no_pretraining\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model = tf.saved_model.load(\"text_cnn_no_pretraining\")\n",
    "y_pred = saved_model(tf.constant([\"this is a terrible movie.\",\"this is a good movie.\",\"very interesting movie\",\"i wouldn't watch this movie.\",\"i recommend this movie.\"]))\n",
    "y_pred"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
