{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"실습6 - Tensorflow Hub로 objection detection.ipynb","private_outputs":true,"provenance":[{"file_id":"1YE2tlwhl4gh12fVnrKwM2FJ9UJYzK9Kk","timestamp":1611654144225}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"98rds-2OU-Rd"},"source":["##### Copyright 2020 The TensorFlow Hub Authors.\n","\n","Licensed under the Apache License, Version 2.0 (the \"License\");"]},{"cell_type":"code","metadata":{"cellView":"form","id":"1c95xMGcU5_Z"},"source":["#@title Copyright 2020 The TensorFlow Hub Authors. All Rights Reserved.\n","#\n","# Licensed under the Apache License, Version 2.0 (the \"License\");\n","# you may not use this file except in compliance with the License.\n","# You may obtain a copy of the License at\n","#\n","#     http://www.apache.org/licenses/LICENSE-2.0\n","#\n","# Unless required by applicable law or agreed to in writing, software\n","# distributed under the License is distributed on an \"AS IS\" BASIS,\n","# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","# See the License for the specific language governing permissions and\n","# limitations under the License.\n","# =============================================================================="],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"V1UUX8SUUiMO"},"source":["<table class=\"tfo-notebook-buttons\" align=\"left\">\n","  <td>\n","    <a target=\"_blank\" href=\"https://www.tensorflow.org/hub/tutorials/tf2_object_detection\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n","  </td>\n","  <td>\n","    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/hub/blob/master/examples/colab/tf2_object_detection.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n","  </td>\n","  <td>\n","    <a target=\"_blank\" href=\"https://github.com/tensorflow/hub/blob/master/examples/colab/tf2_object_detection.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View on GitHub</a>\n","  </td>\n","  <td>\n","    <a href=\"https://storage.googleapis.com/tensorflow_docs/hub/examples/colab/tf2_object_detection.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n","  </td>\n","  <td>\n","    <a href=\"https://tfhub.dev/tensorflow/collections/object_detection/1\"><img src=\"https://www.tensorflow.org/images/hub_logo_32px.png\" />See TF Hub models</a>\n","  </td>\n","</table>"]},{"cell_type":"markdown","metadata":{"id":"rOvvWAVTkMR7"},"source":["# TensorFlow Hub Object Detection Colab\n","\n","Welcome to the TensorFlow Hub Object Detection Colab! This notebook will take you through the steps of running an \"out-of-the-box\" object detection model on images."]},{"cell_type":"markdown","metadata":{"id":"IRImnk_7WOq1"},"source":["### More models\n","[This](https://tfhub.dev/tensorflow/collections/object_detection/1) collection contains TF 2 object detection models that have been trained on the COCO 2017 dataset. [Here](https://tfhub.dev/s?module-type=image-object-detection) you can find all object detection models that are currently hosted on [tfhub.dev](tfhub.dev)."]},{"cell_type":"markdown","metadata":{"id":"vPs64QA1Zdov"},"source":["## Imports and Setup\n","\n","Lets start with the base imports."]},{"cell_type":"code","metadata":{"id":"yn5_uV1HLvaz"},"source":["# 시스템 명령어 처리\n","import os\n","import pathlib\n","\n","# 이미지 처리\n","import matplotlib\n","import matplotlib.pyplot as plt\n","\n","# 파일 처리\n","import io\n","import scipy.misc\n","import numpy as np\n","\n","from six import BytesIO\n","from PIL import Image, ImageDraw, ImageFont\n","from six.moves.urllib.request import urlopen\n","\n","import tensorflow as tf\n","# tensorflow 버전과 상관없음\n","import tensorflow_hub as hub\n","\n","tf.get_logger().setLevel('ERROR')\n","\n","# 아래 Run this!! 반드시 실행!!! \n","# Run this!! 코드를 보고 싶으면 \"텝해서 미러링\" 버튼 클릭!!"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IogyryF2lFBL"},"source":["## Utilities\n","\n","Run the following cell to create some utils that will be needed later:\n","\n","- Helper method to load an image\n","- Map of Model Name to TF Hub handle\n","- List of tuples with Human Keypoints for the COCO 2017 dataset. This is needed for models with keyponts."]},{"cell_type":"code","metadata":{"id":"-y9R0Xllefec"},"source":["# title Run this!!\n","\n","def load_image_into_numpy_array(path):\n","  \"\"\"Load an image from file into a numpy array.\n","\n","  Puts image into numpy array to feed into tensorflow graph.\n","  Note that by convention we put it into a numpy array with shape\n","  (height, width, channels), where channels=3 for RGB.\n","\n","  Args:\n","    path: the file path to the image\n","\n","  Returns:\n","    uint8 numpy array with shape (img_height, img_width, 3)\n","  \"\"\"\n","  image = None\n","  if(path.startswith('http')):\n","    response = urlopen(path)\n","    image_data = response.read()\n","    image_data = BytesIO(image_data)\n","    image = Image.open(image_data)\n","  else:\n","    image_data = tf.io.gfile.GFile(path, 'rb').read()\n","    image = Image.open(BytesIO(image_data))\n","\n","  (im_width, im_height) = image.size\n","  return np.array(image.getdata()).reshape(\n","      (1, im_height, im_width, 3)).astype(np.uint8)\n","\n","\n","ALL_MODELS = {\n","'CenterNet HourGlass104 512x512' : 'https://tfhub.dev/tensorflow/centernet/hourglass_512x512/1',\n","'CenterNet HourGlass104 Keypoints 512x512' : 'https://tfhub.dev/tensorflow/centernet/hourglass_512x512_kpts/1',\n","'CenterNet HourGlass104 1024x1024' : 'https://tfhub.dev/tensorflow/centernet/hourglass_1024x1024/1',\n","'CenterNet HourGlass104 Keypoints 1024x1024' : 'https://tfhub.dev/tensorflow/centernet/hourglass_1024x1024_kpts/1',\n","'CenterNet Resnet50 V1 FPN 512x512' : 'https://tfhub.dev/tensorflow/centernet/resnet50v1_fpn_512x512/1',\n","'CenterNet Resnet50 V1 FPN Keypoints 512x512' : 'https://tfhub.dev/tensorflow/centernet/resnet50v1_fpn_512x512_kpts/1',\n","'CenterNet Resnet101 V1 FPN 512x512' : 'https://tfhub.dev/tensorflow/centernet/resnet101v1_fpn_512x512/1',\n","'CenterNet Resnet50 V2 512x512' : 'https://tfhub.dev/tensorflow/centernet/resnet50v2_512x512/1',\n","'CenterNet Resnet50 V2 Keypoints 512x512' : 'https://tfhub.dev/tensorflow/centernet/resnet50v2_512x512_kpts/1',\n","'EfficientDet D0 512x512' : 'https://tfhub.dev/tensorflow/efficientdet/d0/1',\n","'EfficientDet D1 640x640' : 'https://tfhub.dev/tensorflow/efficientdet/d1/1',\n","'EfficientDet D2 768x768' : 'https://tfhub.dev/tensorflow/efficientdet/d2/1',\n","'EfficientDet D3 896x896' : 'https://tfhub.dev/tensorflow/efficientdet/d3/1',\n","'EfficientDet D4 1024x1024' : 'https://tfhub.dev/tensorflow/efficientdet/d4/1',\n","'EfficientDet D5 1280x1280' : 'https://tfhub.dev/tensorflow/efficientdet/d5/1',\n","'EfficientDet D6 1280x1280' : 'https://tfhub.dev/tensorflow/efficientdet/d6/1',\n","'EfficientDet D7 1536x1536' : 'https://tfhub.dev/tensorflow/efficientdet/d7/1',\n","'SSD MobileNet v2 320x320' : 'https://tfhub.dev/tensorflow/ssd_mobilenet_v2/2',\n","'SSD MobileNet V1 FPN 640x640' : 'https://tfhub.dev/tensorflow/ssd_mobilenet_v1/fpn_640x640/1',\n","'SSD MobileNet V2 FPNLite 320x320' : 'https://tfhub.dev/tensorflow/ssd_mobilenet_v2/fpnlite_320x320/1',\n","'SSD MobileNet V2 FPNLite 640x640' : 'https://tfhub.dev/tensorflow/ssd_mobilenet_v2/fpnlite_640x640/1',\n","'SSD ResNet50 V1 FPN 640x640 (RetinaNet50)' : 'https://tfhub.dev/tensorflow/retinanet/resnet50_v1_fpn_640x640/1',\n","'SSD ResNet50 V1 FPN 1024x1024 (RetinaNet50)' : 'https://tfhub.dev/tensorflow/retinanet/resnet50_v1_fpn_1024x1024/1',\n","'SSD ResNet101 V1 FPN 640x640 (RetinaNet101)' : 'https://tfhub.dev/tensorflow/retinanet/resnet101_v1_fpn_640x640/1',\n","'SSD ResNet101 V1 FPN 1024x1024 (RetinaNet101)' : 'https://tfhub.dev/tensorflow/retinanet/resnet101_v1_fpn_1024x1024/1',\n","'SSD ResNet152 V1 FPN 640x640 (RetinaNet152)' : 'https://tfhub.dev/tensorflow/retinanet/resnet152_v1_fpn_640x640/1',\n","'SSD ResNet152 V1 FPN 1024x1024 (RetinaNet152)' : 'https://tfhub.dev/tensorflow/retinanet/resnet152_v1_fpn_1024x1024/1',\n","'Faster R-CNN ResNet50 V1 640x640' : 'https://tfhub.dev/tensorflow/faster_rcnn/resnet50_v1_640x640/1',\n","'Faster R-CNN ResNet50 V1 1024x1024' : 'https://tfhub.dev/tensorflow/faster_rcnn/resnet50_v1_1024x1024/1',\n","'Faster R-CNN ResNet50 V1 800x1333' : 'https://tfhub.dev/tensorflow/faster_rcnn/resnet50_v1_800x1333/1',\n","'Faster R-CNN ResNet101 V1 640x640' : 'https://tfhub.dev/tensorflow/faster_rcnn/resnet101_v1_640x640/1',\n","'Faster R-CNN ResNet101 V1 1024x1024' : 'https://tfhub.dev/tensorflow/faster_rcnn/resnet101_v1_1024x1024/1',\n","'Faster R-CNN ResNet101 V1 800x1333' : 'https://tfhub.dev/tensorflow/faster_rcnn/resnet101_v1_800x1333/1',\n","'Faster R-CNN ResNet152 V1 640x640' : 'https://tfhub.dev/tensorflow/faster_rcnn/resnet152_v1_640x640/1',\n","'Faster R-CNN ResNet152 V1 1024x1024' : 'https://tfhub.dev/tensorflow/faster_rcnn/resnet152_v1_1024x1024/1',\n","'Faster R-CNN ResNet152 V1 800x1333' : 'https://tfhub.dev/tensorflow/faster_rcnn/resnet152_v1_800x1333/1',\n","'Faster R-CNN Inception ResNet V2 640x640' : 'https://tfhub.dev/tensorflow/faster_rcnn/inception_resnet_v2_640x640/1',\n","'Faster R-CNN Inception ResNet V2 1024x1024' : 'https://tfhub.dev/tensorflow/faster_rcnn/inception_resnet_v2_1024x1024/1',\n","'Mask R-CNN Inception ResNet V2 1024x1024' : 'https://tfhub.dev/tensorflow/mask_rcnn/inception_resnet_v2_1024x1024/1'\n","}\n","\n","IMAGES_FOR_TEST = {\n","  'Beach' : 'models/research/object_detection/test_images/image2.jpg',\n","  'Dogs' : 'models/research/object_detection/test_images/image1.jpg',\n","  # By Heiko Gorski, Source: https://commons.wikimedia.org/wiki/File:Naxos_Taverna.jpg\n","  'Naxos Taverna' : 'https://upload.wikimedia.org/wikipedia/commons/6/60/Naxos_Taverna.jpg',\n","  # Source: https://commons.wikimedia.org/wiki/File:The_Coleoptera_of_the_British_islands_(Plate_125)_(8592917784).jpg\n","  'Beatles' : 'https://upload.wikimedia.org/wikipedia/commons/1/1b/The_Coleoptera_of_the_British_islands_%28Plate_125%29_%288592917784%29.jpg',\n","  # By Américo Toledano, Source: https://commons.wikimedia.org/wiki/File:Biblioteca_Maim%C3%B3nides,_Campus_Universitario_de_Rabanales_007.jpg\n","  'Phones' : 'https://upload.wikimedia.org/wikipedia/commons/thumb/0/0d/Biblioteca_Maim%C3%B3nides%2C_Campus_Universitario_de_Rabanales_007.jpg/1024px-Biblioteca_Maim%C3%B3nides%2C_Campus_Universitario_de_Rabanales_007.jpg',\n","  # Source: https://commons.wikimedia.org/wiki/File:The_smaller_British_birds_(8053836633).jpg\n","  'Birds' : 'https://upload.wikimedia.org/wikipedia/commons/0/09/The_smaller_British_birds_%288053836633%29.jpg',\n","}"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"14bNk1gzh0TN"},"source":["## Visualization tools\n","\n","To visualize the images with the proper detected boxes, keypoints and segmentation, we will use the TensorFlow Object Detection API. To install it we will clone the repo."]},{"cell_type":"code","metadata":{"id":"oi28cqGGFWnY"},"source":["# Clone the tensorflow models repository\n","!git clone --depth 1 https://github.com/tensorflow/models"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yX3pb_pXDjYA"},"source":["Intalling the Object Detection API"]},{"cell_type":"code","metadata":{"id":"NwdsBdGhFanc"},"source":["# 아래는 리눅스 명령어\n","%%bash\n","sudo apt install -y protobuf-compiler\n","cd models/research/\n","protoc object_detection/protos/*.proto --python_out=.\n","cp object_detection/packages/tf2/setup.py .\n","python -m pip install .\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3yDNgIx-kV7X"},"source":["Now we can import the dependencies we will need later"]},{"cell_type":"code","metadata":{"id":"2JCeQU3fkayh"},"source":["from object_detection.utils import label_map_util\n","from object_detection.utils import visualization_utils as viz_utils\n","from object_detection.utils import ops as utils_ops\n","\n","%matplotlib inline"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NKtD0IeclbL5"},"source":["### Load label map data (for plotting).\n","\n","Label maps correspond index numbers to category names, so that when our convolution network predicts `5`, we know that this corresponds to `airplane`.  Here we use internal utility functions, but anything that returns a dictionary mapping integers to appropriate string labels would be fine.\n","\n","We are going, for simplicity, to load from the repository that we loaded the Object Detection API code"]},{"cell_type":"code","metadata":{"id":"5mucYUS6exUJ"},"source":["PATH_TO_LABELS = './models/research/object_detection/data/mscoco_label_map.pbtxt'\n","category_index = label_map_util.create_category_index_from_labelmap(PATH_TO_LABELS, use_display_name=True)\n","# 아래 Model Selection 코드를 보고 싶으면 \"텝해서 미러링\" 버튼 클릭!!"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6917xnUSlp9x"},"source":["## Build a detection model and load pre-trained model weights\n","\n","Here we will choose witch Object Detection model we will use.\n","Select the architecture and it will be loaded automatically.\n","If you want to change the model to try other architectures later, just change the next cell and execute following ones.\n","\n","**Tip:** if you want to read more details about the selected model, you can follow the link (model handle) and read aditional documentation on TF Hub. After you select a model, we will print the handle to make it easier."]},{"cell_type":"code","metadata":{"id":"HtwrSqvakTNn"},"source":["#@title Model Selection { display-mode: \"form\", run: \"auto\" }\n","model_display_name = 'Mask R-CNN Inception ResNet V2 1024x1024' # @param ['CenterNet HourGlass104 512x512','CenterNet HourGlass104 Keypoints 512x512','CenterNet HourGlass104 1024x1024','CenterNet HourGlass104 Keypoints 1024x1024','CenterNet Resnet50 V1 FPN 512x512','CenterNet Resnet50 V1 FPN Keypoints 512x512','CenterNet Resnet101 V1 FPN 512x512','CenterNet Resnet50 V2 512x512','CenterNet Resnet50 V2 Keypoints 512x512','EfficientDet D0 512x512','EfficientDet D1 640x640','EfficientDet D2 768x768','EfficientDet D3 896x896','EfficientDet D4 1024x1024','EfficientDet D5 1280x1280','EfficientDet D6 1280x1280','EfficientDet D7 1536x1536','SSD MobileNet v2 320x320','SSD MobileNet V1 FPN 640x640','SSD MobileNet V2 FPNLite 320x320','SSD MobileNet V2 FPNLite 640x640','SSD ResNet50 V1 FPN 640x640 (RetinaNet50)','SSD ResNet50 V1 FPN 1024x1024 (RetinaNet50)','SSD ResNet101 V1 FPN 640x640 (RetinaNet101)','SSD ResNet101 V1 FPN 1024x1024 (RetinaNet101)','SSD ResNet152 V1 FPN 640x640 (RetinaNet152)','SSD ResNet152 V1 FPN 1024x1024 (RetinaNet152)','Faster R-CNN ResNet50 V1 640x640','Faster R-CNN ResNet50 V1 1024x1024','Faster R-CNN ResNet50 V1 800x1333','Faster R-CNN ResNet101 V1 640x640','Faster R-CNN ResNet101 V1 1024x1024','Faster R-CNN ResNet101 V1 800x1333','Faster R-CNN ResNet152 V1 640x640','Faster R-CNN ResNet152 V1 1024x1024','Faster R-CNN ResNet152 V1 800x1333','Faster R-CNN Inception ResNet V2 640x640','Faster R-CNN Inception ResNet V2 1024x1024','Mask R-CNN Inception ResNet V2 1024x1024']\n","model_handle = ALL_MODELS[model_display_name]\n","\n","print('Selected model:'+ model_display_name)\n","print('Model Handle at TensorFlow Hub: {}'.format(model_handle))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"muhUt-wWL582"},"source":["## Loading the selected model from TensorFlow Hub\n","\n","Here we just need the model handle that was selected and use the Tensorflow Hub library to load it to memory.\n"]},{"cell_type":"code","metadata":{"id":"rBuD07fLlcEO"},"source":["print('loading model...')\n","hub_model = hub.load(model_handle)\n","print('model loaded!')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GIawRDKPPnd4"},"source":["## Loading an image\n","\n","Let's try the model on a simple image. To help with this, we provide a list of test images.\n","\n","Here are some simple things to try out if you are curious:\n","* Try running inference on your own images, just upload them to colab and load the same way it's done in the cell below.\n","* Modify some of the input images and see if detection still works.  Some simple things to try out here include flipping the image horizontally, or converting to grayscale (note that we still expect the input image to have 3 channels).\n","\n","**Be careful:** when using images with an alpha channel, the model expect 3 channels images and the alpha will count as a 4th.\n","\n"]},{"cell_type":"code","metadata":{"id":"hX-AWUQ1wIEr"},"source":["#title Image Selection (don't forget to execute the cell!) { display-mode: \"form\"}\n","selected_image = 'Dogs' # @param ['Beach', 'Dogs', 'Naxos Taverna', 'Beatles', 'Phones', 'Birds']\n","flip_image_horizontally = False #@param {type:\"boolean\"}\n","convert_image_to_grayscale = False #@param {type:\"boolean\"}\n","\n","image_path = IMAGES_FOR_TEST[selected_image]\n","image_np = load_image_into_numpy_array(image_path)\n","\n","# Flip horizontally\n","if(flip_image_horizontally):\n","  image_np[0] = np.fliplr(image_np[0]).copy()\n","\n","# Convert image to grayscale\n","if(convert_image_to_grayscale):\n","  image_np[0] = np.tile(\n","    np.mean(image_np[0], 2, keepdims=True), (1, 1, 3)).astype(np.uint8)\n","\n","plt.figure(figsize=(24,32))\n","plt.imshow(image_np[0])\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FTHsFjR6HNwb"},"source":["## Doing the inference\n","\n","To do the inference we just need to call our TF Hub loaded model.\n","\n","Things you can try:\n","* Print out `result['detection_boxes']` and try to match the box locations to the boxes in the image.  Notice that coordinates are given in normalized form (i.e., in the interval [0, 1]).\n","* inspect other output keys present in the result. A full documentation can be seen on the models documentation page (pointing your browser to the model handle printed earlier)"]},{"cell_type":"code","metadata":{"id":"Gb_siXKcnnGC"},"source":["# running inference\n","# hub_model은 위에서 선택한 모델로 불러오도록 코드를 구성\n","# hub_model에서는 이미지는 4체널로 선언해줘야 하기 때문에 앞에 1차원을 만들어줌\n","print(image_np.shape)  # 이미지 크기 확인\n","# image_np 는  위에 이미지 선택하는 코드에서 4체널로 변경해준다.\n","results = hub_model(image_np)\n","\n","# different object detection models have additional results\n","# all of them are explained in the documentation\n","# tensorflow hub의 사용법은 모델의 결과값을 dict형태로 선언해서 사용하기 때문에 아래 형태로 선언\n","result = {key:value.numpy() for key,value in results.items()}\n","\n","#print(result)  # 실제 값들이 어떻게 구성되어 있는지 보기 위한 출력"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yt-zBVwIbB9v"},"source":["print(result.keys())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gKcWMaoFV-e_"},"source":["print(result['detection_anchor_indices'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Vx5D_TYEiZZA"},"source":["a = result['detection_anchor_indices']\r\n","a_np = a[0].numpy()\r\n","a_list = list(a_np)\r\n","a_sorted = sorted(a_list)\r\n","print(a_sorted)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IZ5VYaBoeeFM"},"source":["## Visualizing the results\n","\n","Here is where we will need the TensorFlow Object Detection API to show the squares from the inference step (and the keypoints when available).\n","\n","the full documentation of this method can be seen [here](https://github.com/tensorflow/models/blob/master/research/object_detection/utils/visualization_utils.py)\n","\n","Here you can, for example, set `min_score_thresh` to other values (between 0 and 1) to allow more detections in or to filter out more detections."]},{"cell_type":"code","metadata":{"id":"2O7rV8g9s8Bz"},"source":["label_id_offset = 0\n","image_np_with_detections = image_np.copy()\n","print(image_np_with_detections.shape)\n","\n","viz_utils.visualize_boxes_and_labels_on_image_array(\n","      image_np_with_detections[0],\n","              # 함수 내부에서 원본 이미지의 크기 확인 및 화면에 그리는 이미지 선언\n","              # 모델 종류에 상관없이 이미지 크기는 원본 그대로 넣어도 된다.\n","              # 자동으로 /255 계산과, resize 코드가 내장되어 있다.\n","              # 원본 이미지값을 사용해야 나중에 화면에 표시할때 원본 크기로 계산할때 필요하기 때문\n","      result['detection_boxes'][0],  # 사각형 좌표 들\n","      (result['detection_classes'][0] + label_id_offset).astype(int),  # class 종류\n","      result['detection_scores'][0],  # 각 사각형에 대한 object인 확률 (0~1 사이)\n","      category_index,  # category 종류를 넣어주면 출력한 class 숫자를 category_index[class]로 출력해준다.\n","      use_normalized_coordinates=True,  # max_boxes_to_draw와 min_score_thresh 사용여부\n","      max_boxes_to_draw=200,  # 최대 200개 까지 그린다.\n","      min_score_thresh=.30,  # score값이 0.3 이하는 제거\n","      agnostic_mode=False)\n","\n","# figure 크기 설정 , 기본보다 크게 보여짐\n","plt.figure(figsize=(24,32))\n","plt.imshow(image_np_with_detections[0])\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Qaw6Xi08NpEP"},"source":["## [Optional]\n","\n","Among the available object detection models there's Mask R-CNN and the output of this model allows instance segmentation.\n","\n","To visualize it we will use the same method we did before but adding an aditional parameter: `instance_masks=output_dict.get('detection_masks_reframed', None)`\n"]},{"cell_type":"code","metadata":{"id":"zl3qdtR1OvM_"},"source":["# Handle models with masks:\n","image_np_with_mask = image_np.copy()\n","print(image_np.shape)\n","print(image_np_with_mask.shape)\n","if 'detection_masks' in result:\n","  # we need to convert np.arrays to tensors\n","  detection_masks = tf.convert_to_tensor(result['detection_masks'][0])\n","  detection_boxes = tf.convert_to_tensor(result['detection_boxes'][0])\n","\n","  # Reframe the the bbox mask to the image size.\n","  detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n","            detection_masks, detection_boxes,\n","              image_np.shape[1], image_np.shape[2])\n","  detection_masks_reframed = tf.cast(detection_masks_reframed > 0.5,\n","                                      tf.uint8)\n","  result['detection_masks_reframed'] = detection_masks_reframed.numpy()\n","\n","viz_utils.visualize_boxes_and_labels_on_image_array(\n","      image_np_with_mask[0],\n","      result['detection_boxes'][0],\n","      (result['detection_classes'][0] + label_id_offset).astype(int),\n","      result['detection_scores'][0],\n","      category_index,\n","      use_normalized_coordinates=True,\n","      max_boxes_to_draw=200,\n","      min_score_thresh=.30,\n","      agnostic_mode=False,\n","      instance_masks=result.get('detection_masks_reframed', None),\n","      line_thickness=8)\n","\n","plt.figure(figsize=(24,32))\n","plt.imshow(image_np_with_mask[0])\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"s6sPujNoAwSr"},"source":["from google.colab import drive\r\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"W3QCc7qInm2V"},"source":["!ls \"/content/drive/My Drive/Colab Notebooks\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RIA3E4uFA4yj"},"source":["import cv2\r\n","video_input_path = \"/content/drive/My Drive/Colab Notebooks/programmers/YOLO.mp4\"\r\n","# linux에서 video output의 확장자는 반드시 avi 로 설정 필요. \r\n","video_output_path = \"/content/drive/My Drive/Colab Notebooks/programmers/YOLO.avi\"\r\n","\r\n","# 동영상 불러오기\r\n","cap = cv2.VideoCapture(video_input_path)\r\n","\r\n","# avi 코덱 선언\r\n","codec = cv2.VideoWriter_fourcc(*'XVID')\r\n","# 동영상 이미지 가로 크기\r\n","width = round(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\r\n","# 동영상 이미지 세로 크기\r\n","height = round(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\r\n","vid_size = (round(cap.get(cv2.CAP_PROP_FRAME_WIDTH)),round(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)))\r\n","# 동영상 초당 영상 재생 개수\r\n","vid_fps = cap.get(cv2.CAP_PROP_FPS)\r\n","# 동영상 저장용 선언\r\n","vid_writer = cv2.VideoWriter(video_output_path, codec, vid_fps, vid_size) \r\n","# 동영상 총 이미지 갯수=총 frame 갯수\r\n","\r\n","frame_cnt = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\r\n","print('총 Frame 갯수:', frame_cnt)\r\n","print('fps:',vid_fps)\r\n","print('총 시간:', int(frame_cnt/vid_fps),'초 = ', int(frame_cnt/vid_fps/60),'분', int(frame_cnt/vid_fps)%60,'초')\r\n","print('가로:', width, '세로:', height)\r\n","\r\n","# 동영상에서 불필요한 장면을 스킵 하기 위해서 비디오 시작지점을 설정한다.\r\n","# cap.set(cv2.CAP_PROP_POS_FRAMES, 40*vid_fps)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Welb29hQ47f2"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fUToRZ8woxg1"},"source":["import time"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-MT0nH3fA9ji"},"source":["def get_hub_model_result(image, _model):\r\n","  # _model은 위에서 선택한 모델로 불러오도록 코드를 구성\r\n","  # 선택한 모델로 찾을 image와 box와 labeling할 이미지  \r\n","  start = time.time()\r\n","  results = _model(image)\r\n","  end = time.time()\r\n","  print(end-start)\r\n","\r\n","  # different object detection models have additional results\r\n","  # all of them are explained in the documentation\r\n","  # tensorflow hub의 사용법은 모델의 결과값을 dict형태로 선언해서 사용하기 때문에 아래 형태로 선언\r\n","  result = {key:value.numpy() for key,value in results.items()}\r\n","  # 시작이 1 인지 0 인지 차이를 선언\r\n","  label_id_offset = 0\r\n","\r\n","  viz_utils.visualize_boxes_and_labels_on_image_array(\r\n","      image[0],  # 함수 내부에서 원본 이미지의 크기 확인 및 화면에 그리는 이미지 선언\r\n","                 # 모델 종류에 상관없이 이미지 크기는 원본 그대로 넣어도 된다.\r\n","                 # 자동으로 /255 계산과, resize 코드가 내장되어 있다.\r\n","                 # 원본 이미지값을 사용해야 나중에 화면에 표시할때 원본 크기로 계산할때 필요하기 때문\r\n","      result['detection_boxes'][0],  # 사각형 좌표 들\r\n","      (result['detection_classes'][0] + label_id_offset).astype(int),  # class 종류\r\n","      result['detection_scores'][0],  # 각 사각형에 대한 object인 확률 (0~1 사이)\r\n","      category_index,  # category 종류를 넣어주면 출력한 class 숫자를 category_index[class]로 출력해준다.\r\n","      use_normalized_coordinates=True, # max_boxes_to_draw와 min_score_thresh 사용여부\r\n","      max_boxes_to_draw=200,  # 최대 200개 까지 그린다.\r\n","      min_score_thresh=.30,  # score값이 0.3 이하는 제거\r\n","      agnostic_mode=False,  # True일 경우 score만 표시, clas 표시 안함, 기본은 False\r\n","      # 아래는 해당 값이 있으면 사용함\r\n","   ) \r\n","  return image[0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EX17KNLaFcwa"},"source":["frame_count = 0\r\n","while True:\r\n","  hasFrame, img_frame = cap.read()\r\n","  print('frame count', frame_count)  \r\n","  if not hasFrame:\r\n","    print('더 이상 처리할 frame이 없습니다.')\r\n","    break  \r\n","\r\n","  # hub_model에서는 이미지는 4체널로 선언해줘야 하기 때문에 앞에 1차원을 만들어줌\r\n","  test_img = np.array(img_frame).reshape((1, height, width, 3)).astype(np.uint8)\r\n","  # test_img : 테스트할 이미지, hub_model : 테스트할 모델\r\n","  draw_img_frame  = get_hub_model_result(test_img, hub_model)\r\n","  # 동영상중에서 1개의 frame 이미지를 진행했다는 count 표시용\r\n","  frame_count += 1\r\n","\r\n","  # 아래는 확인용 코드\r\n","  # plt.figure(figsize=(24,32))  # 크게 보고싶으면 코드 주석 제거\r\n","  # plt.imshow(draw_img_frame)\r\n","  # plt.show()\r\n","  vid_writer.write(draw_img_frame)  \r\n","\r\n","  # 너무 많이 저장하면 오래걸리기 때문에 짧은 시간안에 테스트 할정도로 설정\r\n","  if frame_count > 60: break  \r\n","\r\n","vid_writer.release()\r\n","cap.release()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XHnrgBFrmG05"},"source":["## 3일차 실습\r\n","다른 모델들에 대해도 비디오 파일로 object detection하는 실습을 진행해보기"]},{"cell_type":"code","metadata":{"id":"OQukd-z3mPhI"},"source":[""],"execution_count":null,"outputs":[]}]}